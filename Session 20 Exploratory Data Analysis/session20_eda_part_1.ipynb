{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/google/applied-machine-learning-intensive/blob/master/content/02_data/05_exploratory_data_analysis/colab-part1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"copyright"},"source":["#### Copyright 2020 Google LLC."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jHUQ8PUGZh8y"},"outputs":[],"source":["# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","# https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."]},{"cell_type":"markdown","metadata":{"id":"HQswfu6M_ZBR"},"source":["# Exploratory Data Analysis"]},{"cell_type":"markdown","metadata":{"id":"invE35z3n2Yn"},"source":["[Exploratory Data Analysis](https://en.wikipedia.org/wiki/Exploratory_data_analysis), often shortened to EDA, is a term that you'll hear quite a bit in the field of data science. EDA is the process of examining a dataset to find facts about the data and communicating those facts, often through visualizations.\n","\n","In order to explore the data and visualize it, some modifications might need to be made to the data along the way. This is often referred to as *data preprocessing*. Though data preprocessing is technically different from EDA, EDA often exposes problems with the data that need to be fixed in order to continue exploring. Because of this tight coupling, we'll clean the data as necessary to help understand the data.\n","\n","In this lab we will apply our Pandas knowledge to explore a dataset about chocolate. Part 1 of the lab will explore each column in our dataset individually. Part 2 will take the results of our preprocessed data and search for patterns across columns and rows."]},{"cell_type":"markdown","metadata":{"id":"bIxZkNQzBiyc"},"source":["## Introduction\n","\n"]},{"cell_type":"markdown","metadata":{"id":"U4nDJfscDH9o"},"source":["### The Dataset: Chocolate Bar Ratings\n","\n","In this lab we will use a [chocolate bar ratings dataset](https://www.kaggle.com/rtatman/chocolate-bar-ratings). The dataset is from the [Flavors of Cacao](http://flavorsofcacao.com/flavor.html) data.\n","\n","On the [Kaggle page for the dataset](https://www.kaggle.com/rtatman/chocolate-bar-ratings), we can find some basic information about the dataset. For instance, there are over 1,700 chocolate bars that have been rated. We can also preview the columns found in the dataset:\n","\n","Column | Data Type | Description\n","-------|-----------|-------------\n","Company (Maker-if known) | String | Name of the company manufacturing the bar.\n","Specific Bean Origin or Bar Name | String | The specific geo-region of origin for the bar.\n","REF | Number | A value linked to when the review was entered in the database. Higher = more recent.\n","Review Date | Number | Date of publication of the review.\n","Cocoa Percent | String | Cocoa percentage (darkness) of the chocolate bar being reviewed.\n","Company Location | String | Manufacturer base country.\n","Rating | Number | Expert rating for the bar.\n","BeanType | String | The variety (breed) of bean used, if provided.\n","Broad Bean Origin | String | The broad geo-region of origin for the bean.\n","\n","This is an interesting dataset. Think of the questions that you might be able to answer! A few could be:\n","\n","*   Is there a relationship between numeric rating and properties such as percentage of cocoa, bean type, origin, and maker?\n","*   Are some of the properties of cacao beans correlated?\n","*   Where are the top chocolate bars from?\n","*   Are there multiple entries for the same bar from the same maker, but with different ratings over the years? If so, has there been any change in the chocolate bar that could account for the differences?\n","*   Do makers who produce a wide variety of bars have a higher chance of creating a top-rated chocolate bar?\n","\n","I'm sure you can think of even more. So, what are we waiting for? Let's load the data!"]},{"cell_type":"markdown","metadata":{"id":"GP-kRR_72562"},"source":["## Acquiring the Data\n","\n","The data is hosted on Kaggle, so we can use our Kaggle credentials to download the data into the lab. The dataset is located at [https://www.kaggle.com/rtatman/chocolate-bar-ratings](https://www.kaggle.com/rtatman/chocolate-bar-ratings). We can use the `kaggle` command line utility to do this.\n","\n","First off, upload your `kaggle.json` file into the lab now.\n","\n","Next, run the following command to get the credential files set to the right permissions and located in the correct spot."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-UuViqN232cr"},"outputs":[],"source":["! chmod 600 kaggle.json && (ls ~/.kaggle 2>/dev/null || mkdir ~/.kaggle) && mv kaggle.json ~/.kaggle/ && echo 'Done'"]},{"cell_type":"markdown","metadata":{"id":"gCMEX2915D1Y"},"source":["Now we can run the `kaggle` command to actually download the data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mo1mS1Jp4c5W"},"outputs":[],"source":["! kaggle datasets download rtatman/chocolate-bar-ratings\n","! ls"]},{"cell_type":"markdown","metadata":{"id":"PytnXVWU7Lom"},"source":["We now have our data downloaded to our virtual machine and stored in the file `chocolate-bar-ratings.zip`."]},{"cell_type":"markdown","metadata":{"id":"A8pOhe8_7YOt"},"source":["## Creating a `DataFrame`\n","\n","We now need to load the data into memory. We can do this easily using Pandas' `read_csv()` function."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dEDlbe_L7LEl"},"outputs":[],"source":["import pandas as pd\n","\n","df = pd.read_csv('chocolate-bar-ratings.zip')\n","df"]},{"cell_type":"markdown","metadata":{"id":"BNK6_wRAUK8g"},"source":["Let's also make sure that our data types match what was documented:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"igRLU1b4UOaK"},"outputs":[],"source":["df.dtypes"]},{"cell_type":"markdown","metadata":{"id":"RwASf5wWUQvK"},"source":["In this output, `object` types are strings while `int64` types are whole numbers and `float64` types are fractional numbers. This seems to match the documentation that we saw for the dataset."]},{"cell_type":"markdown","metadata":{"id":"oHYaN8iV7v8_"},"source":["From just a glance at the `DataFrame`, we can see a few facts about our data:\n","\n","* There are 1,795 rows and 9 columns.\n","* The columns are the columns we expected based on the documentation, though some have `\\n` (new line) embedded in them. We'll need to clean that up.\n","* The data seems to be sorted by the 'Company' column.\n","* There is definitely some missing data, as we can see in the 'Bean Type' column.\n","\n","We will look more closely at each column throughout this lab."]},{"cell_type":"markdown","metadata":{"id":"2rQzustf9B4b"},"source":["## Cleaning Up Column Names\n","\n","One of the more frustrating aspects of this dataset is the poor format of the column names. Typing 'Specific Bean Origin\\nor Bar Name' in order to access the column is painful.\n","\n","So our first order of business will be to update the column names."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iBb6frD19ZEp"},"outputs":[],"source":["df.columns = [\n","  'Company',\n","  'Specific Bean Origin',\n","  'REF',\n","  'Review Date',\n","  'Cocoa Percent',\n","  'Company Location',\n","  'Rating',\n","  'Bean Type',\n","  'Broad Bean Origin'\n","]\n","\n","# cara 2\n","# df.rename(colums={'Company':})\n","\n","df"]},{"cell_type":"markdown","metadata":{"id":"Pac1xBZk_TtO"},"source":["That's much better, but the columns are also in an odd order. Information about the company is spread across the columns, and so is the information about the cacao bean. Let's order the columns a little more meaningfully.\n","\n","This order makes a little more sense:\n","\n","**Company Information:**\n","* Company\n","* Company Location\n","\n","**Chocolate Bar Information**\n","* Bean Type\n","* Specific Bean Origin\n","* Broad Bean Origin\n","* Cocoa Percent\n","\n","**Review Information**\n","* REF\n","* Review Date\n","* Rating\n","\n","We can reorder the columns by specifically selecting the columns in order and reassigning them to the `df` variable:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8OzxV-x66YKS"},"outputs":[],"source":["df = df[[\n","  'Company',\n","  'Company Location',\n","  'Bean Type',\n","  'Specific Bean Origin',\n","  'Broad Bean Origin',\n","  'Cocoa Percent',\n","  'REF',\n","  'Review Date',\n","  'Rating',\n","]]\n","\n","df"]},{"cell_type":"markdown","metadata":{"id":"y-gRkwD89Ycv"},"source":["## Examining Each Column\n","\n","\n","In this section we will examine each column to learn about the data in the column. We will also make changes to the data as needed."]},{"cell_type":"markdown","metadata":{"id":"dEe__vNL_pA4"},"source":["### Column: Company"]},{"cell_type":"markdown","metadata":{"id":"uA6heaR7TlmX"},"source":["The 'Company' column is the first in the list, so let's look at it first.\n","\n","We can tell that the column contains string values. Let's see if any are missing:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UWkigEjU_3cn"},"outputs":[],"source":["df['Company'].isnull().any()"]},{"cell_type":"code","source":["df.isnull().sum()"],"metadata":{"id":"Ws54lRTIQxSb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df[df['Bean Type'].isna()]"],"metadata":{"id":"32iqInjyQygn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# df.fillna('', inplace=True)"],"metadata":{"id":"NQTv29z_Qzmc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8qyTNSA3AOWe"},"source":["No data is missing. Let's now see how many distinct values there are:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O-nr1--gATSg"},"outputs":[],"source":["df['Company'].unique().size"]},{"cell_type":"markdown","metadata":{"id":"PjeigrI6AZgo"},"source":["A few hundred is not a terribly long list. Let's print the list in alphabetical order to see how it looks."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_POI-9fCAgAi"},"outputs":[],"source":["for company in sorted(df['Company'].unique()):\n","  print(company)"]},{"cell_type":"markdown","metadata":{"id":"k8E5wwxVCFqd"},"source":["This is some interesting data. Looking at it raises many questions. For instance:\n","\n","* Should company names like 'Vintage Plantations' and 'Vintage Plantations (Tulicorp)' be changed to the same name?\n","* Is 'Cacao de Origin' a misspelling of 'Cacao de Origen'?\n","* Is 'Shattel' a misspelling of 'Shattell'?\n","\n","These are the types of things you'll see and questions you'll ask when you encounter a new dataset. Rarely is the data in perfect condition. Often you'll spend a considerable amount of time researching topics related to the data in order to make a call about repairing aspects of the data.\n","\n","In this particular case, it would be great if we could find a master list of all of the chocolate makers in the world. We could then cross reference the names in the dataset with the names in the master list.\n","\n","Unfortunately, we don't have a master list of chocolate makers. Instead, we will have to rely on manually inspecting the data and researching when things don't look right.\n","\n","Let's say that for now we are confident that 'Cacao de Origin' and 'Shattel' are misspellings, so we will correct that data. We aren't confident enough to change any of the names with parentheses in them though.\n","\n","Let's fix our misspellings!"]},{"cell_type":"markdown","metadata":{"id":"2Vc78yFJFLw5"},"source":["#### Exercise 1: Fixing Misspellings\n","\n","We have decided that we would like to change every instance of 'Cacao de Origin' to 'Cacao de Origen' and every instance of 'Shattel' to 'Shattell' in the 'Company' column of our dataset. Write the code to modify the values. Make sure your code doesn't have any warnings. At the end of the code block, print the number of unique company names when you are done. There should be two less columns than what you saw above."]},{"cell_type":"markdown","metadata":{"id":"Pag-PfyLG69x"},"source":["**Student Solution**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p4pKMDONG9Z4"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","df = pd.read_csv('chocolate-bar-ratings.zip')\n","df.columns = ['Company', 'Specific Bean Origin', 'REF', 'Review Date',\n","              'Cocoa Percent', 'Company Location', 'Rating', 'Bean Type',\n","              'Broad Bean Origin']\n","df = df[['Company', 'Company Location', 'Bean Type', 'Specific Bean Origin',\n","         'Broad Bean Origin', 'Cocoa Percent', 'REF', 'Review Date', 'Rating']]\n","\n","# Change 'Shattel' to 'Shattell'\n","df['Company'] = np.where(df['Company'] == 'Shattel', 'Shattell', df['Company'])\n","\n","# Change 'Cacao de Origin' to 'Cacao de Origen'\n","df['Company'] = np.where(df['Company'] == 'Cacao de Origin', 'Cacao de Origen', df['Company'])\n","\n","# Print the number of unique company names\n","df['Company'].unique().size"]},{"cell_type":"markdown","metadata":{"id":"dIAOmyuWHMa_"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"ac0L7_QP7OTm"},"source":["### Column: Company Location"]},{"cell_type":"markdown","metadata":{"id":"84LQDf9d7OTk"},"source":["The [documentation](https://www.kaggle.com/rtatman/chocolate-bar-ratings) describes the 'Company Location' column as \"*Manufacturer base country*.\"\n","\n","Let's take a look at the data. As always, we'll first check to see if any data is missing."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oTTcRMFq7OTe"},"outputs":[],"source":["df['Company Location'].isna().any()"]},{"cell_type":"code","source":["df.isnull().sum()"],"metadata":{"id":"GXcrOKpBQ6en"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NjjMjZqj7OTc"},"source":["No missing data.\n","\n","Now we can see how many unique values there are:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aSzY7dlr7OTT"},"outputs":[],"source":["df['Company Location'].unique().shape"]},{"cell_type":"code","source":["df['Company Location'].nunique()"],"metadata":{"id":"P5-wceEOQ8D4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MLjkdqps7OTN"},"source":["There are just 60 locations, which is small enough that we can manually inspect the values. Let's print the data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nGFKHE2B7OS5"},"outputs":[],"source":["for location in sorted(df['Company Location'].unique()):\n","  print(location)"]},{"cell_type":"markdown","metadata":{"id":"sJqm2iCU7OSt"},"source":["Overall, the data looks pretty clean. The column is supposed to contain countries and *most* entries are countries. There are a few problems with the country data though. We found at least five errors in the data. Let's see what you can find."]},{"cell_type":"markdown","metadata":{"id":"_d7_RrV_-DPP"},"source":["#### Exercise 2: Fixing Company Location Data\n","\n","There are at least five errors in the company location data that need to be fixed. Some are fairly easy to spot (spelling errors), but some do require knowledge of what constitutes a country. Take some time to look at the data, and see if you can spot at least two of the issues. Write code to fix the issues."]},{"cell_type":"markdown","metadata":{"id":"p2uvJNxo-DD8"},"source":["**Student Solution**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1eBSJfCp-C4-"},"outputs":[],"source":["# Fix at least two issues with the 'Company Location' data\n","df['Company Location'] = np.where(df['Company Location'] == 'Niacragua', 'Nicaragua', df['Company Location'])\n","df['Company Location'] = np.where(df['Company Location'] == 'Eucador', 'Ecuador', df['Company Location'])\n","# Domincan Republic, UK, Amsterdam"]},{"cell_type":"markdown","metadata":{"id":"s5toiHcB-CvH"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"nA_FefG8dtJQ"},"source":["### Column: Bean Type"]},{"cell_type":"markdown","metadata":{"id":"4ibiKPxvCCoA"},"source":["Now that our company data is looking a little better, let's move into data about the cocoa going into the chocolate bar itself. The first piece of data is the 'Bean Type'. 'Bean Type' is defined as \"*The variety (breed) of bean used, if provided*\". This hints that there will be some missing data. Let's check and see."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ulcp5RH0CbJs"},"outputs":[],"source":["df['Bean Type'].isna().any()"]},{"cell_type":"markdown","metadata":{"id":"ER7siTrsCe4m"},"source":["Indeed, we have missing data. Let's see how much is missing."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NEqYfWsxCnlt"},"outputs":[],"source":["df[df['Bean Type'].isna()].count()"]},{"cell_type":"markdown","metadata":{"id":"CZwRD78JC6sK"},"source":["Only one row of data is missing 'Bean Type'. Let's take a look at that row."]},{"cell_type":"code","source":["- MNAR: Not missing at random.\n","- MAR: Missing at random.\n","- MCAR: Missing completely at random."],"metadata":{"id":"T-6Mlyr0RCmT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# MNAR\n","ID Tipe_Rumah Jumlah_Halaman Jumlah_Kamar\n","1 HOME 1 3\n","2 HOME 1 4\n","3 APARTMENT NULL 1\n","4 HOME 1 2\n","5 APARMENT NULL 2\n","6 APARTMENT NULL 3\n","7 APARTMENT NULL 4\n","\n","# MAR\n","ID Tipe_Rumah Jumlah_Halaman Jumlah_Kamar\n","1 HOME 1 3\n","2 HOME 1 4\n","3 APARTMENT NULL 1\n","4 HOME 1 2\n","5 APARMENT NULL 2\n","6 APARTMENT NULL 3\n","7 APARTMENT 1 4\n","\n","# MCAR\n","ID Tipe_Rumah Jumlah_Halaman Jumlah_Kamar\n","1 HOME 1 3\n","2 HOME 1 4\n","3 APARTMENT 1 1\n","4 HOME 1 2\n","5 APARMENT NULL 2\n","6 APARTMENT 1 3\n","7 APARTMENT 1 4"],"metadata":{"id":"U8w8ajccRI_I"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZvSyoal2DBCH"},"outputs":[],"source":["df[df['Bean Type'].isna()]"]},{"cell_type":"markdown","metadata":{"id":"SWnKcJ5HDF5B"},"source":["Now we have a choice to make about how to handle this missing data. Some options include:\n","\n","* Leave it as is\n","* Remove the entire row\n","* Fill in the data with some value\n","\n","Leaving undefined values lying around in our data can be problematic. Missing values are not counted and can be tricky to program around.\n","\n","Removing the entire row actually isn't a bad option in this case. Since it is only one row out of over 1,700, it likely won't have too much effect on any analysis that we do.\n","\n","As for filling in the row, we can:\n","\n","* Use 'Unknown' or some other placeholder value\n","* Actually do research to find the true missing value\n","* See if there is a reasonable value already in the data\n","\n","In this case, we are just going to replace the missing value with 'Unknown'."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qCVeiD-2E3MN"},"outputs":[],"source":["df.loc[df['Bean Type'].isna(), 'Bean Type'] = 'Unknown'\n","df[df['Bean Type'].isna()]"]},{"cell_type":"markdown","metadata":{"id":"jaoPvnZnHMjg"},"source":["Now we can see how many unique bean types we have."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dj_ttYmwFATj"},"outputs":[],"source":["df['Bean Type'].unique().size"]},{"cell_type":"markdown","metadata":{"id":"-PJNVBJAHUgK"},"source":["Only 42, let's print them out."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EGITpas9HXwH"},"outputs":[],"source":["for t in sorted(df['Bean Type'].unique()):\n","  print(t)"]},{"cell_type":"markdown","metadata":{"id":"BY0UqpgsHjTu"},"source":["The data looks pretty good. But there is a small little problem. After 'Unknown' there seems to be an empty line. What is that?\n","\n","It turns out that it is a whitespace character. We thought we had only one missing value, but it looks like there are some values that are present but are white space. Let's see how many.\n","\n","White space can be tricky because there are many different encodings that render as white space. Let's find out exactly which space character this is.\n","\n","To get the space(s) we can sort the 'Bean Type' values again and get the last one, since we see the space last in the list. We can then print the space as hexadecimal characters."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O2E_EcvCH-NH"},"outputs":[],"source":["space = sorted(df['Bean Type'].unique())[-1]\n","print(\", \".join(\"0x{:02x}\".format(ord(c)) for c in space))"]},{"cell_type":"markdown","metadata":{"id":"0TTbLvZSJUZw"},"source":["We get `0xa0` which is the ASCII code for [non-breaking space](https://en.wikipedia.org/wiki/Non-breaking_space). This is different from the white space that you get when you hit the space bar. That space is encoded `0x20`.\n","\n","Let's see how many of these there are:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KDOkj9BTJspc"},"outputs":[],"source":["df[df['Bean Type'] == chr(0xa0)]"]},{"cell_type":"markdown","metadata":{"id":"E6iQWvqCK9fM"},"source":["Almost 900! Let's encode those as 'Unknown' also."]},{"cell_type":"markdown","metadata":{"id":"xkf_tZeLLECc"},"source":["#### Exercise 3: Fixing Non-Breaking Space\n","\n","There are non-breaking space characters, `0xa0` in the 'Bean Type' column. Replace these values with the word 'Unknown'."]},{"cell_type":"markdown","metadata":{"id":"eSXdAvnuLSUT"},"source":["**Student Solution**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8FGsZXdiLUYJ"},"outputs":[],"source":["# Your Code Goes Here\n","df['Bean Type'] = np.where(df['Bean Type'] == chr(0xa0), 'Unknown', df['Bean Type'])"]},{"cell_type":"code","source":["for bt in sorted(df['Bean Type'].unique()):\n","  print(bt)"],"metadata":{"id":"beYL2lFERSmn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iToRkZ8pLWb4"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"f8c5MvFFHh_4"},"source":["### Column: Specific Bean Origin"]},{"cell_type":"markdown","metadata":{"id":"6MAc6snPJ6LK"},"source":["Let's look at our next column: 'Specific Bean Origin'. 'Specific Bean Origin' is a string column that contains the \"*specific geo-region of origin for the bar*.\"\n","\n","First, we'll see if we are missing any data in the 'Specific Bean Origin' column."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e9Lk5uA_RKN5"},"outputs":[],"source":["df['Specific Bean Origin'].isna().any()"]},{"cell_type":"markdown","metadata":{"id":"DQh-ivg2RNfS"},"source":["Good, we don't have any 'N/A' data. But we learned from the 'Bean' column that we also need to check string columns for being only white space.\n","\n","A good way to do this is to apply a function that strips leading and trailing white space from every value in a column, and see if the resulting string is zero-length."]},{"cell_type":"code","source":["x = ' Cacao de  Origen '\n","x.strip()"],"metadata":{"id":"NIWtRZMCRU-n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x = '                '\n","x.strip()"],"metadata":{"id":"75gOEopeRWMh"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9-MLWbPgMxZm"},"outputs":[],"source":["df[df['Specific Bean Origin'].apply(lambda x: x.strip()).str.len() == 0]"]},{"cell_type":"markdown","metadata":{"id":"Oxyh7ZFKPEJY"},"source":["Here we can see that no data was returned, so we don't have any 'Specific Bean Origin' values that are only spaces.\n","\n","If you run this function and get an error about numbers/floats not having a strip function, you likely have N/A values in your column. Always check `isna()` first.\n","\n","Now that we know that every row has a 'Specific Bean Origin' value, let's see how many unique values we have."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MTfjVmwmLMZM"},"outputs":[],"source":["df['Specific Bean Origin'].unique().size"]},{"cell_type":"markdown","metadata":{"id":"TxRFy8RnLXVk"},"source":["Over 1,000 values! That is quite a bit of data to manually sift through. Let's look at the first bit of data, up until the first origin that starts with 'B'."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ibDslHr2HrTw"},"outputs":[],"source":["for origin in sorted(df['Specific Bean Origin'].unique()):\n","  if origin.startswith('B'):\n","    break\n","  print(origin)"]},{"cell_type":"markdown","metadata":{"id":"X7otjUTJMUhb"},"source":["This is some pretty ugly data. Most (but not all) rows contain the bean's geographical origin, but some seem to include the year and/or batch numbers as well, and some seem to contain different information entirely (\"100 percent\").\n","\n","Looking at the data, we can also see some things that look odd. For instance, \"Akesson Estate\" and \"Akesson's Estate\" are likely the same origin. Also, \"Ambolikapkly P.\" clearly looks like a misspelling of \"Ambolikapiky P.\"\n","\n","We could make all of the \"Akesson\" origins look the same, but should we? First, let's look at the entire rows for the offending data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"74DvVEDvNgXF"},"outputs":[],"source":["df[(df['Specific Bean Origin'] == 'Akesson Estate') | \\\n","   (df['Specific Bean Origin'] == \"Akesson's Estate\")]"]},{"cell_type":"markdown","metadata":{"id":"htWHPgw3OUJE"},"source":["It is interesting that all of the bean types and origins are alike. It looks like Akesson('s) Estate serves many companies though.\n","\n","It is tempting to go ahead and change the \"Specific Bean Origin\" values to make them match, but it is better to do more research into the industry before making those sorts of changes. You might disagree with this decision, and that is perfectly fine. When working with datasets, you will often have to make difficult calls to deal with ambiguous data. Different people will make different decisions, and that's okay.\n","\n","The \"Ambolikapkly P.\" issue is a little more obvious and can be validated with a quick internet search. The \"Ambolikapkly\" spelling shows up very few times and always in the context of this data set. The other spelling is much more common. Let's go ahead and fix that."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4XB8MPWLO0ZC"},"outputs":[],"source":["df.loc[df['Specific Bean Origin'] == 'Ambolikapkly P.', \n","       'Specific Bean Origin'] = 'Ambolikapiky P.'"]},{"cell_type":"markdown","metadata":{"id":"Lrm76h5vPORJ"},"source":["#### Exercise 4: Finding and Repairing Bad Data\n","\n","There are a few more obvious errors in the 'Specific Bean Origin' column of the dataset. Print out the column, scan the output, and see if you can find any more errors. Write the code to fix the errors. Find at least one error to fix.\n","\n","The code to print the dataset is below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0yYMKfmIPqOh"},"outputs":[],"source":["for origin in sorted(df['Specific Bean Origin'].unique()):\n","  print(origin)"]},{"cell_type":"markdown","metadata":{"id":"JhuNx5G5R47g"},"source":["**Student Solution**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hxoy0g2iR79D"},"outputs":[],"source":["# Repair the data\n","df['Specific Bean Origin'] = np.where(df['Specific Bean Origin'] == 'Bolivian', 'Bolivia', df['Specific Bean Origin'])"]},{"cell_type":"markdown","metadata":{"id":"YFZQoDvHR_Rb"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"9dyeOcIASyBa"},"source":["#### Exercise 5: Top Specific Bean Origins\n","\n","There are just over 1,000 unique specific bean origins and over 1,700 entries in the dataset. Write code to find the top five most repeated origins. Print the origins and the number of times that each appears in the dataset."]},{"cell_type":"markdown","metadata":{"id":"yLz75cOFTE_T"},"source":["**Student Solution**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ew8c5AknTHuQ"},"outputs":[],"source":["# Find the top 5 bar origins\n","df.groupby('Specific Bean Origin')['Specific Bean Origin'].count().sort_values(ascending=False).head(n=5)"]},{"cell_type":"markdown","metadata":{"id":"0abZ4gATTGtJ"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"OUPRZpCIdvT8"},"source":["### Column: Broad Bean Origin"]},{"cell_type":"markdown","metadata":{"id":"lI5_PqH7Tqq3"},"source":["The 'Broad Bean Origin' is the \"*broad geo-region of origin for the bean.*\" In theory, this should be broader regions than the 'Specific Bean Origin' that we just worked with.\n","\n","Let's dive in. First things first, let's check for N/A values."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-xfNxS3NUVRw"},"outputs":[],"source":["df[df['Broad Bean Origin'].isna()].count()"]},{"cell_type":"markdown","metadata":{"id":"PDAtc4e6cmot"},"source":["It looks like we are missing one origin. Let's take a look at the record."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yua9pGNhdOcP"},"outputs":[],"source":["df[df['Broad Bean Origin'].isna()]"]},{"cell_type":"markdown","metadata":{"id":"UUz2RFhHde9o"},"source":["The one record has a 'Specific Bean Origin' of 'Madagascar'. Let's see if there are any other chocolates from that same specific origin."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NWwhBGNGdRK7"},"outputs":[],"source":["df[df['Specific Bean Origin'] == 'Madagascar']"]},{"cell_type":"markdown","metadata":{"id":"N2dcIgIqdr__"},"source":["Quite a few! And they all have a 'Broad Bean Origin' of 'Madagascar', except for our one missing value. It is probably safe to just set the missing value to 'Madagascar' also."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4QYJkAFOd8sn"},"outputs":[],"source":["df.loc[(df['Specific Bean Origin'] == 'Madagascar') &\n","       (df['Broad Bean Origin'].isna()),\n","       'Broad Bean Origin'] = 'Madagascar'\n","\n","df[df['Broad Bean Origin'].isna()]"]},{"cell_type":"markdown","metadata":{"id":"q9hd_1sUePTY"},"source":["Now that we have all of the N/A values handled, let's see if we have an issue with spaces."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fbeQTVRYeUpK"},"outputs":[],"source":["df[df['Broad Bean Origin'].apply(lambda x: x.strip()).str.len() == 0]"]},{"cell_type":"markdown","metadata":{"id":"aH6kvNxEeb3T"},"source":["There are spaces in 73 rows of the data. Let's see what those space values are."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GNT-nh6sehTp"},"outputs":[],"source":["spaces_df = df[df['Broad Bean Origin'].apply(\n","    lambda x: x.strip()).str.len() == 0]\n","\n","for space in spaces_df['Broad Bean Origin'].unique():\n","  print(\", \".join(\"0x{:02x}\".format(ord(c)) for c in space))"]},{"cell_type":"markdown","metadata":{"id":"DHekK-OUe9sm"},"source":["It is that pesky `0xa0` again.\n","\n","We can fix this by replacing all of the `0xa0` values with 'Unknown'. However, an even better fix would be if we could find similar chocolates with the same 'Specific Bean Origin' and then derive the 'Broad Bean Origin' from that.\n","\n","Let's see if it is even possible. To do that we can find all of the 'Specific Bean Origin' values for rows with 'Broad Bean Origin' and those without. Then we can use `pd.merge()` to combine the two. If you remember, `pd.merge()` returns only the values which appear in both of the given Series. This means that the return value will show us which values appear both in columns with 'Broad Bean Origin' values and those without."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3QFKBDMMr-kz"},"outputs":[],"source":["has_bbo_idx = df['Broad Bean Origin'].apply(lambda x: x.strip()).str.len() > 0\n","\n","sbo_bbo = df[has_bbo_idx]['Specific Bean Origin']\n","sbo_no_bbo = df[~has_bbo_idx]['Specific Bean Origin']\n","\n","pd.merge(sbo_bbo, sbo_no_bbo)"]},{"cell_type":"markdown","metadata":{"id":"pv2qhp-stnDK"},"source":["We have overlap, which is good. In theory, we could use the 'Broad Bean Origin' values from bars that *have* that value to fill in the 'Broad Bean Origin' for bars from the same specific region that *don't have* it.\n","\n","But look closely at those 'Specific Bean Origin' values. Dark? Raw? Blend?\n","\n","Those are specific origins. The only two origins that seem even close to regions are 'Amazonas' and 'Orinoco'. Let's look closer at the data for those regions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E4Rwap-emG7I"},"outputs":[],"source":["df[(df['Specific Bean Origin'] == 'Orinoco') | \n","   (df['Specific Bean Origin'] == 'Amazonas')]"]},{"cell_type":"markdown","metadata":{"id":"OLd2P63Cur58"},"source":["Yuck! Amazonas turns out to be a very common location. There are states called Amazonas in Brazil, Venezuela, and Peru. Orinoco is a river that runs through both Venezuela and Columbia.\n","\n","In neither case do we have definitive data to make the call about the 'Broad Bean Origin' for these rows.\n","\n","Unfortunately that is how it goes when working with data. You get imperfect data into your system, and then you try to research and find the best fix. But you sometimes just have to accept that you are missing data."]},{"cell_type":"markdown","metadata":{"id":"rp4vPDsrvXOd"},"source":["#### Exercise 6: Unknown Broad Bean Origins\n","\n","We have a few 'Broad Bean Origin' values of `0xa0`. Change those values to the literal string 'Unknown'."]},{"cell_type":"markdown","metadata":{"id":"Am8-XSgUvnTV"},"source":["**Student Solution**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w-1nztOJvqU0"},"outputs":[],"source":["# Your Code Goes Here\n","df['Broad Bean Origin'] = np.where(df['Broad Bean Origin'] == chr(0xa0), 'Unknown', df['Broad Bean Origin'])"]},{"cell_type":"markdown","metadata":{"id":"eTGo8qRPvpTY"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"u62i8AyedjYS"},"source":["### Column: Cocoa Percent"]},{"cell_type":"markdown","metadata":{"id":"R9yoBTgVn2FC"},"source":["Next we will check out the 'Cocoa Percent' column. Remember that 'Cocoa Percent' is \"*Cocoa percentage (darkness) of the chocolate bar*.\"\n","\n","As usual, we'll first see if there is any missing data:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yf9ZR67MO2On"},"outputs":[],"source":["df['Cocoa Percent'].isna().any()"]},{"cell_type":"markdown","metadata":{"id":"obWQn5v_O6Y8"},"source":["Nothing missing. Great!\n","\n","Next, we should probably check to make sure that the percentages fall within a valid range: 0-100 or 0.0-1.0. You might recall that 'Cocoa Percent' isn't actually a numeric column, though, so we can't easily find the range. If we sample the data, we see that it looks like percentages from 0 to 100, but they are stored as strings with '%' symbols appended."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PpefxWtCPSLg"},"outputs":[],"source":["df['Cocoa Percent'].sample(10)"]},{"cell_type":"markdown","metadata":{"id":"0pM0VQeZPw7K"},"source":["We need to remove those percentage signs and convert the digits that remain into numbers. There are a few ways that we can accomplish this.\n","\n","One is to apply a lambda to each value. The lambda can slice all but the last character of each value and then convert it to a float using core Python syntax."]},{"cell_type":"code","source":["s = '125%'\n","s[:-1]"],"metadata":{"id":"-3rxwAbhRtYf"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Ck7RSOBP4eV"},"outputs":[],"source":["df['Cocoa Percent'].apply(lambda s: float(s[:-1]))"]},{"cell_type":"markdown","metadata":{"id":"xbgR_ICHRGxA"},"source":["An alternative is to use `.str.strip('%')` on the `Series` to remove the percentage sign and then pass the resultant `Series` to `pd.to_numeric()` in order to convert the string values to numbers."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"73zhUkgbPRL_"},"outputs":[],"source":["pd.to_numeric(df['Cocoa Percent'].str.strip('%'))"]},{"cell_type":"markdown","metadata":{"id":"DEka63nyRWLG"},"source":["Is one way better than the other? Not necessarily. Feel free to choose whichever feels more natural to you.\n","\n","Either way, we need to do the conversion and save the new values to 'Cocoa Percent'."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PDHkS3d_Rpyq"},"outputs":[],"source":["df['Cocoa Percent'] = df['Cocoa Percent'].apply(lambda s: float(s[:-1]))\n","df['Cocoa Percent'].describe()"]},{"cell_type":"markdown","metadata":{"id":"K0v69PWOmpBp"},"source":["We have now converted our 'Cocoa Percent' column from a string to a floating point number. We can see in the output of the call to `describe()` that the minimum cocoa percentage that we have is 42% and that the maximum is 100%. Both seem like reasonable values for cocoa content in a chocolate bar, so our work here is done."]},{"cell_type":"markdown","metadata":{"id":"tdmZTO2KTexy"},"source":["### Column: REF"]},{"cell_type":"markdown","metadata":{"id":"OTwhzLEZTyi8"},"source":["The 'REF' column is \"*A value linked to when the review was entered in the database. Higher = more recent*.\" Let's take a look at it.\n","\n","As always, we should check and see if there are any values missing."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1wHAtxeCVAqD"},"outputs":[],"source":["df['REF'].isna().any()"]},{"cell_type":"markdown","metadata":{"id":"ecfkqlT8VRcY"},"source":["We can `describe()` the data to see some basic statistics about it."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qknt0ahdVH4W"},"outputs":[],"source":["df['REF'].describe()"]},{"cell_type":"markdown","metadata":{"id":"0tufVwglVVwY"},"source":["Here we can see that the data ranges from 5 through 1952 and that the mean is pretty high.\n","\n","Are the values unique?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i-6iGJD0VyBf"},"outputs":[],"source":["df['REF'].unique().size"]},{"cell_type":"markdown","metadata":{"id":"b0W32mH0WP8u"},"source":["Not unique. So 'REF' isn't a unique identifier for our rows of data."]},{"cell_type":"markdown","metadata":{"id":"s2qh6_MGWdQ4"},"source":["There isn't much more that we can do with this column. We might want to visualize it to see if we can find any meaning. The numbers themselves aren't particularly interesting, but the quantity of each number might be. Let's find and plot the count of each 'REF'."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b9zw2eTHWxJB"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","ref_counts = df['REF'].groupby(df['REF']).count()\n","plt.figure(figsize=(20,10))\n","plt.bar(ref_counts.index.values, ref_counts)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"_j-xZZC_XcY7"},"source":["From this chart we can see that 'REF' values repeat between 1 and 9 times with 4 being the most common. Overall, there isn't much interesting data or data repair for this column."]},{"cell_type":"markdown","metadata":{"id":"ceaB_NaubW1x"},"source":["### Column: Review Date"]},{"cell_type":"markdown","metadata":{"id":"nOgnWofebcQx"},"source":["Review date is the date that the review for a given row was actually published. It is a numeric column.\n","\n","First, let's see if any data is missing."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bx71DI97brba"},"outputs":[],"source":["df['Review Date'].isna().any()"]},{"cell_type":"markdown","metadata":{"id":"357tRDRpb1v3"},"source":["No missing data. Good.\n","\n","Now we can check some basic statistics about the data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nTjNEeYub6np"},"outputs":[],"source":["df['Review Date'].describe()"]},{"cell_type":"markdown","metadata":{"id":"e8t6UURbb9QW"},"source":["We can see publication dates ranging from 2006 through 2017, which seems like reasonable years. If we had seen dates from the 1800s or the future, we should be worried. This range seems well within reason, though.\n","\n","There isn't much else that we need to do for this column. Since we only have a few years when reviews were posted, we can create a visualization showing how many reviews were posted each year."]},{"cell_type":"markdown","metadata":{"id":"ftNn3yb5cj2e"},"source":["#### Exercise 7: Reviews Per Year\n","\n","Create a visualization that shows the number of reviews that were created each year."]},{"cell_type":"markdown","metadata":{"id":"h9b2S5Dkct1w"},"source":["**Student Solution**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YGsp-igoczGx"},"outputs":[],"source":["df"]},{"cell_type":"code","source":["df_grouped_by_year = df.groupby(df['Review Date'])[['Review Date']].count()\n","df_grouped_by_year.columns=['Count']\n","df_grouped_by_year.reset_index(inplace=True)\n","df_grouped_by_year"],"metadata":{"id":"TSql8Ph2Shyi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import seaborn as sns\n","plt.figure(figsize=(12,6))\n","plt.title('Reviews per year')\n","sns.lineplot(x='Review Date', y='Count', data=df_grouped_by_year, marker='o')\n","plt.xlabel('Year')\n","plt.ylabel('# of reviews')\n","plt.show()"],"metadata":{"id":"Q9HOyINMSbMG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T-or1La-c6y7"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"mX_nMXjMdrVT"},"source":["### Column: Rating"]},{"cell_type":"markdown","metadata":{"id":"k_GIPUeC08Gz"},"source":["We have now made it to the rating column. The rating is the \"*expert rating for the bar*.\"  From the [documentation](https://www.kaggle.com/rtatman/chocolate-bar-ratings), the possible ratings are:\n","\n","Rating | Meaning\n","-------|---------\n","5 | Elite (Transcending beyond the ordinary limits)\n","4 | Premium (Superior flavor development, character and style)\n","3 | Satisfactory (3.0) to praiseworthy(3.75) (well made with special qualities)\n","2 | Disappointing (Passable but contains at least one significant flaw)\n","1 | Unpleasant (mostly unpalatable)\n","\n","Let's take a look at ratings. First off, are any missing?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mbpdYmLt3vam"},"outputs":[],"source":["df['Rating'].isna().any()"]},{"cell_type":"markdown","metadata":{"id":"Vgj0-juX30CC"},"source":["Nothing missing. Let's describe the column of data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ux0NkE4X33A-"},"outputs":[],"source":["df['Rating'].describe()"]},{"cell_type":"markdown","metadata":{"id":"I8bbhS454MRY"},"source":["It looks like our ratings are indeed floating point values and that they range from 1.0 to 5.0. But are they really continuous?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M8Sh68jo4TPO"},"outputs":[],"source":["sorted(df['Rating'].unique())"]},{"cell_type":"markdown","metadata":{"id":"06Xu3zAoyI1R"},"source":["Interestingly enough, the values don't seem to be continuous, but instead seem to be divided into quarters. Instead of infinite possible values between 1.0 and 5.0, we really have 17 possible values: 1.0, 1.25, 1.5, 1.75, 2.0, 2.25, 2.5, 2.75, 3.0, 3.25, 3.5, 3.75, 4.0, 4.25, 4.5, 4.75, 5.0.\n","\n","What does this mean for us?\n","\n","It means that downstream we might be able to use a regression or categorical model in order to predict these values.\n","\n","If we think about the ratings numbers, their relative position matters. For example, a 4.0 chocolate is better than a 2.0 chocolate. But does the magnitude matter? Is a 4.0 chocolate twice as good as a 2.0 chocolate? What does that even mean?\n","\n","Let's set our modelers up for success and create a new column that they can use to potentially build models for our data."]},{"cell_type":"markdown","metadata":{"id":"31Ca5wI4zqA_"},"source":["#### Exercise 8: Ratings as Categories\n","\n","In this exercise we are going to create a new column called 'Grade'. Grade is a categorical rating system that maps the following ratings to grades:\n","\n","Rating | Grade\n","-------|------\n","5.00   | A\n","4.75   | B\n","4.50   | C\n","4.25   | D\n","4.00   | E\n","3.75   | F\n","3.50   | G\n","3.25   | H\n","3.00   | I\n","2.75   | J\n","2.50   | K\n","2.25   | L\n","2.00   | M\n","1.75   | N\n","1.50   | O\n","1.25   | P\n","1.00   | Q\n","\n","Create the 'Grade' column and add it to our chocolate bar `DataFrame`."]},{"cell_type":"markdown","metadata":{"id":"l8taMl3U0cv2"},"source":["**Student Solution**"]},{"cell_type":"code","source":["df"],"metadata":{"id":"rlvr0XvtTuC1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import string\n","# Your Code Goes Here\n","rating_s = np.arange(1.00, 5.25, 0.25)\n","\n","# Make a series from 'A' to 'Q'\n","grade_s = list(string.ascii_lowercase)[:len(rating_s)]\n","# map_rating_to_grade = pd.Series(rating_s, index=grade_s)\n","# map_rating_to_grade\n","\n","# Make a dictionary mapping from rating_s to grade_s\n","map_rating_to_grade = dict(zip(rating_s, grade_s))\n","map_rating_to_grade"],"metadata":{"id":"WjuDsKuVTvMn"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3ZvVNGDe0cNw"},"outputs":[],"source":["# Your Code Goes Here\n","df['Grade'] = df['Rating'].map(map_rating_to_grade)\n","df"]},{"cell_type":"markdown","metadata":{"id":"sOKQItqF0gR3"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"e0E6_seMAPNg"},"source":["## Onward\n","\n","We've now explored every column in our dataset. We have filled in missing values and repaired obviously bad data.\n","\n","As you can imagine, you can spend near infinite time trying to get a dataset into shape for analysis and modeling. It is common to hear that up to 60% to 80% of a data scientist's time is spent working on the data before it is fed to a model!\n","\n","In this lab, we only tried to get the data into the state that it was intended to be in. Once we get into modeling, we will learn even more data manipulation techniques that need to be used in order to get models to train well on the data.\n","\n","But we aren't quite ready for model building yet. There is still more Exploratory Data Analysis (EDA) to do. In part 2 of this unit, we will look more closely at the relationships between the columns."]}],"metadata":{"colab":{"collapsed_sections":["copyright"],"name":"session20_eda_part_1.ipynb","private_outputs":true,"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}