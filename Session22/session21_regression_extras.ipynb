{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "copyright"
   },
   "source": [
    "#### Copyright 2020 Google LLC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vnxEkIngTvDE"
   },
   "outputs": [],
   "source": [
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9x3myQjxtiaT"
   },
   "source": [
    "# Regression Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jfaowiejr34"
   },
   "source": [
    "So far in this course, we have spent some time building and testing regression models. But how can we measure how good these models are? In this Colab, we will examine a few of the ways that we can measure and graph the results of a regression model in order to better understand the quality of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QpMw1wUayDuO"
   },
   "source": [
    "## Building a Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NiioKUOUyHne"
   },
   "source": [
    "In order to discuss regression quality, we should probably start by building a regression model.\n",
    "\n",
    "We will start by creating an artificial dataset to model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TKHnU1b6zQ_H"
   },
   "source": [
    "Start by importing [NumPy](http://numpy.org) and setting a random seed so that we get reproducible results.\n",
    " \n",
    "Remember: **Do not set a random seed in production code!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EDe_GHXFz08q"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(0xFACADE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kgR1_rQNzP5M"
   },
   "source": [
    "Recall that linear regression is about trying to fit a straight line through a set of data points. The equation for a straight line is:\n",
    "\n",
    "> $y = m*x + b$\n",
    "\n",
    "where:\n",
    "- $x$ is the feature\n",
    "- $y$ is the outcome\n",
    "- $m$ is the slope of the line\n",
    "- $b$ is the intercept of the line on the $y$-axis\n",
    "\n",
    "But at this point we don't even have $x$-values!\n",
    "\n",
    "We will use NumPy's [random.uniform](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.random.uniform.html) function to generate 50 random numbers between 0 and 200 as $x$-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H1GbL97E0an3"
   },
   "outputs": [],
   "source": [
    "X = np.random.uniform(low=0, high=200, size=50)\n",
    "\n",
    "print(f'min: {np.min(X)}')\n",
    "print(f'max: {np.max(X)}')\n",
    "print(f'mean: {np.mean(X)}')\n",
    "print(f'count: {np.size(X)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iUrlpwQw3x8W"
   },
   "source": [
    "You should see a:\n",
    "\n",
    "  * minimum value near, but not below 0\n",
    "  * maximum value near, but not above 200\n",
    "  * mean value somewhere near 100\n",
    "  * count value of exactly 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Vhj5pXD4wnY"
   },
   "source": [
    "Let's visualize the $x$-values, just to get some idea of the distribution of the values in the range of 0-200.\n",
    " \n",
    "*How do we plot a one-dimensional list of values in two-dimensional space?*\n",
    " \n",
    "We can plot it against itself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h46h-Zhn5WSW"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(X, X, 'g.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "okVippyp8rEj"
   },
   "source": [
    "As you can see, we have a straight line of $x$-values that span from roughly 0 to 200. Let's now create some $y$-values via the equation $y=4x+10$ (i.e. the slope is 4 and the intercept is 10).\n",
    "\n",
    "We'll call the new variable `Y_PRED` since it is the predicted variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bxxq56Sa827e"
   },
   "outputs": [],
   "source": [
    "SLOPE = 4\n",
    "INTERCEPT = 10\n",
    "\n",
    "Y_PRED = (SLOPE * X) + INTERCEPT\n",
    "\n",
    "plt.plot(X, Y_PRED, 'b.')\n",
    "plt.plot(X, Y_PRED, 'r-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5lvGtzEjFljz"
   },
   "source": [
    "This regression line fits amazingly well! If only we could have this perfect of a fit in the real world. Unfortunately, this is almost never the case. There is always some variability.\n",
    " \n",
    "Let's add some randomness into our $y$-values to get a more realistic dataset. We will keep our original $y$-values in order to remember our base regression line.\n",
    " \n",
    "We'll recreate our original $y$-values and store them in `Y_PRED`. Then, we'll create `Y` with the same equation but with added randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cX8YZU4YF632"
   },
   "outputs": [],
   "source": [
    "Y_PRED = (SLOPE * X) + INTERCEPT\n",
    "\n",
    "randomness = np.random.uniform(low=-200, high=200, size=50)\n",
    "Y = SLOPE * X + randomness + INTERCEPT\n",
    "\n",
    "plt.plot(X, Y, 'b.')\n",
    "plt.plot(X, Y_PRED, 'r-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dZa4VEpwAIr4"
   },
   "source": [
    "We now have the line that was used to generate the data plotted in red, and the randomly displaced data points in blue. The dots, though definitely not close to the line, at least follow the linear trend in general. This seems like a reasonable dataset for a linear regression.\n",
    " \n",
    "Let's remind ourselves of the key variables in the model:\n",
    " \n",
    "* `X`: the $x$-values that we used to \"train\" the model\n",
    "* `Y`: the $y$-values that represent the actual values that correlate to `X`\n",
    "* `Y_PRED`: the $y$-values that the model would predict for each $x$-value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CXka6VEAgb88"
   },
   "source": [
    "## Coefficient of Determination\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WBHq0fVmCnDt"
   },
   "source": [
    "The **coefficient of determination**, denoted $R^2$, is one of the most important metrics in regression. It tells us how much of the data is \"explained\" by the model.\n",
    "\n",
    "Before we can define the metric itself, we need to define a few other key terms.\n",
    " \n",
    "A **residual** is the difference between the target value $y_i$ and the predicted value $\\hat{y_i}$. The **residual sum of squares** is the summation of the square of every residual in the prediction set.\n",
    " \n",
    "> $$ SS_{res} = \\sum_{i}(y_i - \\hat{y_i})^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ajpg_LjzXlfE"
   },
   "outputs": [],
   "source": [
    "ss_res = ((Y - Y_PRED) ** 2).sum(axis=0,  dtype=np.float64)\n",
    "print(ss_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "txBMdrddXuLT"
   },
   "source": [
    "The **total sum of squares** is the sum of the squares of the difference between each value $y_i$ and their mean.\n",
    "\n",
    "> $$\\bar{y} = \\frac{1}{n}\\sum_{i=1}^{n}y_{i}$$\n",
    "\n",
    "> $$SS_{tot} = \\sum_{i}(y_{i}-\\bar{y})^2$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u4fgXukDaRc-"
   },
   "outputs": [],
   "source": [
    "y_mean = np.average(Y, axis=0)\n",
    "print(y_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tQ5bBjDjbNur"
   },
   "outputs": [],
   "source": [
    "ss_tot = ((Y - y_mean)**2).sum(axis=0, dtype=np.float64)\n",
    "print(ss_tot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BVxzNIVAbaDE"
   },
   "source": [
    "Given the total sum of squares and the residual sum of squares, we can calculate the coefficient of determination $R^2$.\n",
    "\n",
    ">  $$R^{2} = 1 - \\frac{SS_{res}}{SS_{tot}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YIAfSpJGcDff"
   },
   "outputs": [],
   "source": [
    "r2 = 1 - (ss_res/ss_tot)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BKdi_0xGe2IV"
   },
   "source": [
    "If you just ran the cells in this Colab from top to bottom you probably got a score of `0.846`.\n",
    " \n",
    "Is this good? Bad? Mediocre?\n",
    " \n",
    "The $R^2$ score measures how well the actual variance from $x$-values to $y$-values is represented in the variance between the $x$-values and the predicted $y$-values.\n",
    " \n",
    "Typically, this score ranges from 0 to 1, where 0 is bad and 1 is a perfect mapping. However, the score can also be negative. Can you guess why?\n",
    " \n",
    "If a line drawn horizontally through the data points performs better than your regression, then the $R^2$ score would be negative. If you see this, try again. Your model *really* isn't working.\n",
    " \n",
    "For values in the range 0-1, interpreting the $R^2$ is more subjective. The closer to 0, the worse your model is at fitting the data. And generally, the closer to 1, the better (but you also don't want to overfit). This is where testing, observation, and experience come into play.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cJZnpcHagxCC"
   },
   "source": [
    "It turns out that scikit-learn can calculate $R^2$ for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RpEhA5Njhl9o"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "print(r2_score(Y, Y_PRED))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pZTuYycChyVt"
   },
   "source": [
    "Knowing that we don't have to manually do all of the math again, let's now see the perfect and a very imperfect case of a regression fitting a dataset.\n",
    "\n",
    "To begin with, we'll show a perfect fit. What happens if our predictions and our actual values are identical?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hY_YUP5riHt1"
   },
   "outputs": [],
   "source": [
    "print(r2_score(Y, Y))\n",
    "print(r2_score(Y_PRED, Y_PRED))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VTVw_ljAiNrx"
   },
   "source": [
    "1.0: just what we thought! A perfect fit.\n",
    "\n",
    "Now let's see if we can make a regression so poor that $R^2$ is negative.\n",
    "\n",
    "In this case, we need to make our predicted data look different than our actuals. To do this, we'll negate our predictions and save them into a new variable called `Y_PRED_BAD`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RQ8k2wTXigmw"
   },
   "outputs": [],
   "source": [
    "Y_PRED_BAD = -Y_PRED\n",
    "plt.plot(X, Y, 'y.')\n",
    "plt.plot(X, Y_PRED_BAD, 'r-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8SSxTRBzkKb7"
   },
   "source": [
    "That prediction line looks horrible! Indeed, a horizontal line would fit this data better.\n",
    "\n",
    "Let's check the $R^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GsRgZmePjk5R"
   },
   "outputs": [],
   "source": [
    "print(r2_score(Y, Y_PRED_BAD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1H5ilPeOkZNb"
   },
   "source": [
    "A negative $R^2$ is rare in practice. But if you do ever see one, it means the model has gone quite wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u-i-5MvVkuFa"
   },
   "source": [
    "## Predicted vs. Actual Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2d9FGT_2kx7T"
   },
   "source": [
    "We have now seen a quantitative way to measure the goodness-of-fit of our regressions: the coefficient of determination. We know that if we see negative numbers that our model is very broken and if we see numbers approaching 1, the model is decent (or overfitted). But what about the in-between?\n",
    " \n",
    "This is where qualitative observations based on expert opinion needs to come into play.\n",
    "\n",
    "There are numerous ways to visualize regression predictions, but one of the most basic is the \"predicted vs. actual\" plot.\n",
    " \n",
    "To create this plot, we scatter-plot the actual $y$-values used to train our model against the predicted $y$-values generated from the training features. We then draw a line from the lowest prediction to the largest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JP-6LYKf9A3B"
   },
   "outputs": [],
   "source": [
    "plt.plot(Y_PRED, Y, 'b.')\n",
    "plt.plot([Y_PRED.min(), Y_PRED.max()], [Y_PRED.min(), Y_PRED.max()], 'r-')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OP4Kv5ei-ard"
   },
   "source": [
    "In this case, the data points scatter pretty evenly around the prediction-to-actual line.\n",
    "\n",
    "So what does a bad plot look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "me3c2Erb_X2O"
   },
   "source": [
    "Let's first negate all of our predictions, making them exactly the opposite of what they should be. This creates the exact opposite of a good actual-vs-predicted line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wRTjN69E-xim"
   },
   "outputs": [],
   "source": [
    "Y_BAD = -Y_PRED\n",
    "\n",
    "plt.plot(Y, Y_BAD, 'b.')\n",
    "plt.plot([Y_BAD.min(), Y_BAD.max()], [Y_BAD.min(), Y_BAD.max()], 'r-')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nhhr0S92DOA5"
   },
   "source": [
    "In this case we made a very contrived example where the predictions are exact opposites of the actual values. When you see this case, you have a model predicting roughly the opposite of what it should be predicting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PGOGmkemJnCX"
   },
   "source": [
    "Let's look at another case, where we add a large positive bias to every prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ed32rF6x_zMp"
   },
   "outputs": [],
   "source": [
    "Y_BAD = Y_PRED + 200\n",
    "\n",
    "plt.plot(Y, Y_BAD, 'b.')\n",
    "plt.plot([Y_BAD.min(), Y_BAD.max()], [Y_BAD.min(), Y_BAD.max()], 'r-')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RXVkiilyET2d"
   },
   "source": [
    "Now we have a situation where there is an obvious **bias**. All predictions are higher than the actual values, so the model needs to be adjusted to make smaller predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ansCNhhYKNw1"
   },
   "source": [
    "Most cases aren't quite so obvious. In the chart below, you can see that the predictions are okay for low values, but tend to underpredict for larger target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QK6O49mNJwOj"
   },
   "outputs": [],
   "source": [
    "Y_BAD = Y_PRED - Y_PRED / 4\n",
    "\n",
    "plt.plot(Y, Y_BAD, 'b.')\n",
    "plt.plot([Y_BAD.min(), Y_BAD.max()], [Y_BAD.min(), Y_BAD.max()], 'r-')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-5xH6zg4Kjgn"
   },
   "source": [
    "Predicted vs. actual charts are a useful tool for giving you a visual indication of how your model is performing. While single measures like $R^2$ give you an aggregated metric, charts allow you to see if there is a trend or outlier where your model isn't performing well.\n",
    "\n",
    "If you identify problem areas, you can work on retraining your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mkw2D-iaEhmk"
   },
   "source": [
    "## Residual Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zhvs7DF4Eltq"
   },
   "source": [
    "Another helpful visualization tool is to plot the regression residuals. As a reminder, residuals are the difference between the actual values and the predicted values.\n",
    " \n",
    "We plot residuals on the $y$-axis against the predicted values on the $x$-axis, and draw a horizontal line through $y=0$.\n",
    " \n",
    "Cases where our predictions were too low are above the line. Cases where our predictions were too high are below the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xryYLARW2UdB"
   },
   "outputs": [],
   "source": [
    "RESIDUALS = Y - Y_PRED\n",
    "plt.plot(Y_PRED, RESIDUALS, 'b.')\n",
    "plt.plot([0, Y_PRED.max()], [0, 0], 'r-')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Residual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VlWKb7TxPA29"
   },
   "source": [
    "In the \"Predicted vs. Actual\" section above, we plotted a case where there was a large positive bias in our predictions. Plotting the same biased data on a residual plot shows all of the residuals below the zero line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5RGaU1vKOxHQ"
   },
   "outputs": [],
   "source": [
    "RESIDUALS = Y - (Y_PRED + 200)\n",
    "\n",
    "plt.plot(Y_PRED, RESIDUALS, 'b.')\n",
    "plt.plot([0, Y_PRED.max()], [0, 0], 'r-')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Residual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-BZWwqiPPoZr"
   },
   "source": [
    "The other example in the \"Predicted vs. Actual\" section reduced our predictions by an amount proportional to the scale of the predictions. Below is the residual plot for that scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZzcQwgHdPUNo"
   },
   "outputs": [],
   "source": [
    "RESIDUALS = Y - (Y_PRED - Y_PRED / 4)\n",
    "\n",
    "plt.plot(Y_PRED, RESIDUALS, 'b.')\n",
    "plt.plot([0, Y_PRED.max()], [0, 0], 'r-')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Residual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NrSCeh9kCwCy"
   },
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RlLLhik8CzIq"
   },
   "source": [
    "* [Coefficient of Determination](https://en.wikipedia.org/wiki/Coefficient_of_determination)\n",
    "* [Interpreting Residual Plots](http://docs.statwing.com/interpreting-residual-plots-to-improve-your-regression/#gallery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xj-D3cpPK-bR"
   },
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lVRIsmu7LAO0"
   },
   "source": [
    "The [Interpreting Residual Plots](http://docs.statwing.com/interpreting-residual-plots-to-improve-your-regression/#gallery) resource gives examples of patterns in different residual plots and what those patterns might mean for your model.\n",
    " \n",
    "Each exercise below contains code that generates an image. Run the code to view the image, and then find the corresponding pattern name in [Interpreting Residual Plots](http://docs.statwing.com/interpreting-residual-plots-to-improve-your-regression/#gallery). Note the name of the pattern in the answer cell, and provide a one or two sentence explanation of what this could signal about your model's predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CcD-4FQALDGS"
   },
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TVqArWSCLFk8"
   },
   "source": [
    "Run the code below to generate an image. Identify the corresponding residual plot pattern, and write a sentence or two about what this could signal about the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VSDFGtHFWlcy"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "x = np.linspace(-10.0, 10.0, 100)\n",
    "y = np.linspace(-10.0, 10.0, 100)\n",
    "f = x**2 + y**2 + np.random.uniform(low=-14, high=14, size=100)\n",
    "plt.plot(x, f, 'b.')\n",
    "plt.plot([x.min(), x.max()], [0, 0], 'r-')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Residual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xp2Nb65QLHp2"
   },
   "source": [
    "### **Student Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HzRwd-OSXAwb"
   },
   "source": [
    "*Which [plot pattern](http://docs.statwing.com/interpreting-residual-plots-to-improve-your-regression/#gallery) does this residual plot follow? And what might it mean about the model?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9v3ah3zBTzhg"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FmdVEMObLahP"
   },
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BgUTg70ULf14"
   },
   "source": [
    "Run the code below to generate an image. Identify the corresponding residual plot pattern, and write a sentence or two about what this could signal about the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LUaR6zDyXmeJ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "x = np.linspace(0.0, 10.0, 100)\n",
    "y = np.concatenate([\n",
    "    np.random.uniform(low=-5, high=5, size=90),\n",
    "    np.random.uniform(low=50, high=55, size=10)\n",
    "])\n",
    "plt.plot(x, y, 'b.')\n",
    "plt.plot([x.min(), x.max()], [0, 0], 'r-')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Residual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MjMuMUpmLjpv"
   },
   "source": [
    "### **Student Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b6l1tsCXLyZO"
   },
   "source": [
    "*Which [plot pattern](http://docs.statwing.com/interpreting-residual-plots-to-improve-your-regression/#gallery) does this residual plot follow? And what might it mean about the model?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5HnwCb2aT7Ly"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GTYAV7aXY8Y4"
   },
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K62c7ShmY_S4"
   },
   "source": [
    "Run the code below to generate an image. Identify the corresponding residual plot pattern, and write a sentence or two about what this could signal about the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JBZ40HiTZBxH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "x = np.concatenate([\n",
    "    np.random.uniform(low=0, high=2, size=90),\n",
    "    np.random.uniform(low=4, high=10, size=10)\n",
    "])\n",
    "y = np.random.uniform(low=-5, high=5, size=100)\n",
    "plt.plot(x, y, 'b.')\n",
    "plt.plot([x.min(), x.max()], [0, 0], 'r-')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Residual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ciCms7duZrtO"
   },
   "source": [
    "### **Student Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1OihReV6ZvGb"
   },
   "source": [
    "*Which [plot pattern](http://docs.statwing.com/interpreting-residual-plots-to-improve-your-regression/#gallery) does this residual plot follow? And what might it mean about the model?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GZjI0o4tLxkI"
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "copyright",
    "exercise-1-key-1",
    "exercise-2-key-1",
    "kvzJqzO0Zxmq"
   ],
   "name": "Regression Quality",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
